---
# Display name
title: Yansheng Mao

# Full name (for SEO)
first_name: Yansheng
last_name: Mao

# Username (this should match the folder name)
authors:
  - Yansheng Mao

# Is this the primary user of the site?
superuser: false

# Role/position
role: Undergraduate Student

# Organizations/Affiliations
organizations:
  - name: Peking University
    url: 'https://www.pku.edu.cn'

# Short bio (displayed in user profile at end of posts)
bio: Undergraduate student interested in LLMs, Pretraining, and Continual learning.

interests:
  - LLMs
  - Pretraining
  - Continual learning

education:
  courses:
    - course: B.S. in Computer Science, Peking University (Expected 2026)
      institution: Peking University, EECS
      year: 2026

# Social/Academic Networking
# For available icons, see: https://docs.hugoblox.com/getting-started/page-builder/#icons
social:
  - icon: home
    icon_pack: fas
    link: https://luckyglass.github.io
  - icon: envelope
    icon_pack: fas
    link: 'mailto:ysmao_lumen@outlook.com'
  - icon: github
    icon_pack: fab
    link: https://github.com/LuckyGlass
#   - icon: google-scholar
#     icon_pack: ai
#     link: https://scholar.google.com/citations?user=Rmo7EXQAAAAJ
  # - icon: linkedin
  #   icon_pack: fab
  #   link: https://www.linkedin.com/

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: 'student@pku.edu.cn'

# Highlight the author in author lists? (true/false)
highlight_name: false

# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.
user_groups:
  - Undergraduate Students
---


I am Yansheng Mao, an undergraduate student majoring in Computer Science at Peking University, expecting to graduate in 2026. After that, I will continue my studies at Peking University as a Ph.D. student in Artificial Intelligence. Currently, I am a member of Mu Lab, where I am fortunate to be advised by Prof. Muhan Zhang. My research interests broadly span the field of Large Language Models (LLMs), with a primary focus on model pre-training and continual learning. In my recent work, I have been investigating methods for enhancing the capability of LLMs to process and reason over long texts.

More generally, I am interested in advancing the foundations of LLMs and exploring how training paradigms can be improved to achieve more efficient adaptation and sustained performance over time.